## ”optimization” (优化问题)

神经网络属于梯度下降法这个分支中的一个

##  误差方程 (Cost Function)

用来计算预测出来的和我们实际中的值有多大差别. 

## 全局最优解(Global minima)和局部最优(Local minima)

全局最优固然是最好, 但是很多时候, 你手中的都是一个局部最优解, 这也是无可避免的. 不过你可以不必担心, 因为虽然不是全局最优, 但是神经网络也能让你的局部最优足够优秀, 以至于即使拿着一个局部最优也能出色的完成手中的任务.

## 特征(features)和代表特征(feature representation)

![image-20240703194430077](https://github.com/mintonight/mintonight.github.io/assets/115227322/e374b4f3-3886-40ce-bb07-1513b999356a)

在专业术语中, 我们将宝宝当做特征(features), 将神经网络第一层加工后的宝宝叫做代表特征(feature representation). 如果再次移动红线, 我们的黑盒就消失了, 这时原本在黑盒里的所有神经层都被照亮. 原本的代表特征再次被加工, 变成了另一种代表特征, 

## 迁移学习

![](https://static.mofanpy.com/static/results/ML-intro/feature_representation7.png)

对于一个有分类能力的神经网络, 有时候我们只需要这套神经网络的理解能力, 并拿这种能力去处理其他问题. 所以我们保留它的代表特征转换能力. 

比如刚刚我们说将手写数字变成的3个点信息. 然后我们需要干点坏事, 将这个神经网络的输出层给拆掉. 套上另外一个神经网络, 用这种移植的方式再进行训练, 让它处理不同的问题, 比如, 预测照片里事物的价值. 现在看来, 这黑盒里开个灯, 

